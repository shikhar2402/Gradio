import pyarrow.parquet as pq
import pandas as pd
import boto3

# Define your AWS credentials and S3 bucket details
aws_access_key_id = 'YOUR_ACCESS_KEY_ID'
aws_secret_access_key = 'YOUR_SECRET_ACCESS_KEY'
s3_bucket_name = 'your-s3-bucket'
s3_prefix = 'truncated_data/'

# Fetch data from Parquet file in S3
s3 = boto3.client('s3', aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key)
object_key = 'path/to/your/data.parquet'  # Replace with the actual path to your Parquet file in the S3 bucket
parquet_file = pq.ParquetDataset(f's3://{s3_bucket_name}/{object_key}')
table = parquet_file.read().to_pandas()

# Truncate the VARCHAR column
column_to_truncate = 'description'  # Replace with the actual column name
max_length = 65535

table[column_to_truncate] = table[column_to_truncate].apply(lambda x: x[:max_length])

# Convert DataFrame back to Arrow Table
arrow_table = pq.Table.from_pandas(table)

# Write the modified Arrow Table back to Parquet file
modified_parquet_path = 'modified_data.parquet'  # Replace with your desired modified Parquet file name
pq.write_table(arrow_table, f's3://{s3_bucket_name}/{s3_prefix}{modified_parquet_path}', use_dictionary=False)

print(f'Modified data has been written to s3://{s3_bucket_name}/{s3_prefix}{modified_parquet_path}')
